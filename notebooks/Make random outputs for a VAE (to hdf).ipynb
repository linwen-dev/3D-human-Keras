{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is hacked together because I never expected anyone to use it. You can skip it and decompress the data I've commited to the repository. Then you can use main with that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T01:02:38.211471Z",
     "start_time": "2017-04-18T09:02:38.018483+08:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install tqdm\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "from tqdm import tqdm\n",
    "from path import Path\n",
    "import json\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# basedir=r'../vendor/makehuman-commandline/makehuman'\n",
    "basedir=Path('.').abspath()\n",
    "# os.chdir(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:59:05.921410Z",
     "start_time": "2017-04-18T08:59:05.917433+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-04-18T01:03:34.534Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\MakeHuman\\VAE_2.0\\scripts\\wrap_mh\\..\\..\\vendor/makehuman-commandline/makehuman\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized logging\n",
      "compiled file missing: data/3dobjs/base.npz\n",
      "Problem loading binary mesh: ('compiled file missing: %s', u'data/3dobjs/base.npz')\n",
      "Loading ASCII mesh data/3dobjs/base.obj.\n",
      "Not writing compiled meshes to system paths (data/3dobjs/base.npz).\n",
      "Loading material from file data/skins/default.mhmat\n",
      "compiled file missing: data/3dobjs/base.npz\n",
      "Problem loading binary mesh: ('compiled file missing: %s', u'data/3dobjs/base.npz')\n",
      "Loading ASCII mesh data/3dobjs/base.obj.\n",
      "Not writing compiled meshes to system paths (data/3dobjs/base.npz).\n",
      "Loading material from file data/skins/default.mhmat\n",
      "Loading modifiers from data/modifiers/modeling_modifiers.json\n",
      "Attempting to load targets from NPZ file.\n",
      "Could not load targets from NPZ, loading individual files from data (Error message: ('Could not load load targets from npz archive. Archive file %s not found.', u'data\\\\targets.npz'))\n",
      "1258 targets loaded from .target files.\n",
      "Loaded 249 modifiers from file data/modifiers/modeling_modifiers.json\n",
      "Loaded description for head/head-skinny|fat but modifier does not exist!\n",
      "Loaded 248 modifier descriptions from file data/modifiers/modeling_modifiers_desc.json\n",
      "No description defined for modifier head/head-fat-decr|incr!\n"
     ]
    }
   ],
   "source": [
    "wrap_mh_path = basedir.joinpath('..','scripts')\n",
    "os.sys.path.append(str(wrap_mh_path))\n",
    "import wrap_mh\n",
    "from wrap_mh.convert.convert_obj_three import (ALIGN, SHADING, BAKE_COLORS, TRUNCATE, TEMPLATE_FILE_ASCII, SCALE, \n",
    "    generate_materials_string, generate_morph_colors, extract_material_colors, generate_uv, generate_normal,\n",
    "    generate_face, generate_face, generate_color_decimal, generate_vertex, generate_morph_targets, get_name,\n",
    "    parse_obj)\n",
    "# add mh lic info\n",
    "from makehuman import LicenseInfo\n",
    "mh_licence=LicenseInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:34.126316Z",
     "start_time": "2017-04-18T08:57:34.124150+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from path import Path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import makehuman-cmd\n",
    "\n",
    "https://bitbucket.org/duststorm01/makehuman-commandline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:37.456524Z",
     "start_time": "2017-04-18T08:57:35.066044+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized logging\n",
      "compiled file missing: data/3dobjs/base.npz\n",
      "Problem loading binary mesh: ('compiled file missing: %s', u'data/3dobjs/base.npz')\n",
      "Loading ASCII mesh data/3dobjs/base.obj.\n",
      "Not writing compiled meshes to system paths (data/3dobjs/base.npz).\n",
      "Loading material from file data/skins/default.mhmat\n",
      "compiled file missing: data/3dobjs/base.npz\n",
      "Problem loading binary mesh: ('compiled file missing: %s', u'data/3dobjs/base.npz')\n",
      "Loading ASCII mesh data/3dobjs/base.obj.\n",
      "Not writing compiled meshes to system paths (data/3dobjs/base.npz).\n",
      "Loading material from file data/skins/default.mhmat\n",
      "Loading modifiers from data/modifiers/modeling_modifiers.json\n",
      "Loaded 249 modifiers from file data/modifiers/modeling_modifiers.json\n",
      "Loaded description for head/head-skinny|fat but modifier does not exist!\n",
      "Loaded 248 modifier descriptions from file data/modifiers/modeling_modifiers_desc.json\n",
      "No description defined for modifier head/head-fat-decr|incr!\n"
     ]
    }
   ],
   "source": [
    "# from .mh_helpers import clean, short_hash, clean_modifier\n",
    "\n",
    "mhpath = Path(os.path.abspath(\"../vendor/makehuman-commandline/makehuman\"))\n",
    "\n",
    "#===============================================================================\n",
    "# Import Makehuman resources, needs to be with makehuman dir as current dir\n",
    "#===============================================================================\n",
    "\n",
    "appcwd = os.path.abspath(os.curdir)\n",
    "sys.path.append(mhpath)\n",
    "sys.path.append(appcwd)\n",
    "sys.path.append('.')\n",
    "\n",
    "def getHuman():\n",
    "    \"\"\"Load a human model with modifiers.\"\"\"\n",
    "    with mhpath:\n",
    "        # maxFaces *uint* Number of faces per vertex (pole), None for default (min 4)\n",
    "        human = Human(files3d.loadMesh(\n",
    "            getpath.getSysDataPath(\"3dobjs/base.obj\"),\n",
    "            maxFaces=5))\n",
    "        # load modifiers onto human\n",
    "        humanmodifier.mods_loaded = False\n",
    "        modifiers = humanmodifier.loadModifiers(\n",
    "            getpath.getSysDataPath('modifiers/modeling_modifiers.json'), human)\n",
    "        return human\n",
    "\n",
    "with mhpath:\n",
    "    import makehuman\n",
    "    oldpath = os.sys.path\n",
    "    makehuman.set_sys_path()\n",
    "    # make makehuman paths absolute by going through newest paths and making abs\n",
    "    for i in range(len(os.sys.path)):\n",
    "        p = os.sys.path[i]\n",
    "        if p[0:2] == './':\n",
    "            os.sys.path[i] = os.path.join(\n",
    "                os.path.abspath('.'), p.replace('./', ''))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    makehuman.init_logging()\n",
    "    #import image_pil as image_lib\n",
    "    #\n",
    "    import proxy as mhproxy\n",
    "    import humanargparser\n",
    "    import targets as mhtargets\n",
    "    from human import Human\n",
    "    import files3d\n",
    "    import getpath\n",
    "    import humanmodifier\n",
    "    from core import G\n",
    "    import headless\n",
    "    import autoskinblender\n",
    "    import export\n",
    "    \n",
    "    # Init console app\n",
    "    with mhpath:\n",
    "        G.app = headless.ConsoleApp()\n",
    "    G.app.selectedHuman = human = getHuman()\n",
    "    headless.OBJExporter = None\n",
    "    headless.MHXExporter = None\n",
    "    headless.MhxConfig = None\n",
    "    humanargparser.mods_loaded = False\n",
    "\n",
    "def assignModifierValues(human, valuesDict):\n",
    "    _tmp = human.symmetryModeEnabled\n",
    "    human.symmetryModeEnabled = False\n",
    "    for mName, val in valuesDict.items():\n",
    "        try:\n",
    "            human.getModifier(mName).setValue(val)\n",
    "        except:\n",
    "            pass\n",
    "    human.updateMacroModifiers()\n",
    "    human.applyAllTargets()\n",
    "    human.symmetryModeEnabled = _tmp\n",
    "    return human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:37.538220Z",
     "start_time": "2017-04-18T08:57:37.457930+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import proxies\n",
    "def _listDataFiles(foldername,\n",
    "                   extensions,\n",
    "                   onlySysData=False,\n",
    "                   recursive=True):\n",
    "    with mhpath:  # sadly makehuman seems hardcoded\n",
    "        if onlySysData:\n",
    "            paths = [getpath.getSysDataPath(foldername)]\n",
    "        else:\n",
    "            paths = [getpath.getDataPath(foldername),\n",
    "                     getpath.getSysDataPath(foldername)]\n",
    "    return list(getpath.search(paths, extensions, recursive))\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"Remove invalid characters.\"\"\"\n",
    "    s = re.sub('[^0-9a-zA-Z_]', '_', s)\n",
    "    return s\n",
    "\n",
    "with mhpath:\n",
    "    mhproxy.ProxyTypes\n",
    "    proxies = OrderedDict()\n",
    "    for proxyType in mhproxy.ProxyTypes:\n",
    "        files = list(_listDataFiles(proxyType.lower(),\n",
    "                                         ['.proxy', '.mhclo']))\n",
    "        for f in files:\n",
    "            if proxyType not in proxies.keys():\n",
    "                proxies[proxyType] = OrderedDict()\n",
    "            filesname = clean(os.path.splitext(os.path.basename(f))[0])\n",
    "            proxies[proxyType][filesname] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:37.542552Z",
     "start_time": "2017-04-18T08:57:37.539662+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old dupicate imports\n",
    "# import wrap_mh\n",
    "# import proxy\n",
    "# from human import Human\n",
    "# import getpath\n",
    "# from pprint import pprint\n",
    "# import files3d\n",
    "# import humanargparser\n",
    "# import humanmodifier\n",
    "\n",
    "# mhpath=wrap_mh.mhpath\n",
    "\n",
    "\n",
    "# add mh lic info\n",
    "# from makehuman import LicenseInfo\n",
    "# mh_licence=LicenseInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:37.553789Z",
     "start_time": "2017-04-18T08:57:37.543699+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# silence mh\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make random outputs for a VAE\n",
    "\n",
    "TODO\n",
    "- tidy\n",
    "- I'm saving -morphTargets, so flip it around next time I generate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export all modifiers as obj's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:39.664380Z",
     "start_time": "2017-04-18T08:57:37.554887+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxies to choose\n",
      "[u'female1605', u'female_generic', u'male1591', u'proxy741']\n",
      "runname 20170420-223432_None\n"
     ]
    }
   ],
   "source": [
    "# choose proxymesh\n",
    "proxyname= None #'female1605'\n",
    "print 'Proxies to choose'\n",
    "pprint(proxies['Proxymeshes'].keys())\n",
    "export_helpers=False\n",
    "\n",
    "if proxyname:\n",
    "    proxyfile=proxies['Proxymeshes'][proxyname]\n",
    "else:\n",
    "    proxyfile=None\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "runname=timestr+'_'+str(proxyname)\n",
    "print 'runname',runname\n",
    "\n",
    "basehuman=getHuman()\n",
    "with mhpath:\n",
    "    if proxyname:\n",
    "        pxy = proxy.loadTextProxy(basehuman, proxyfile, type='Proxymesh')\n",
    "    else:\n",
    "        pxy = None\n",
    "if proxyname:\n",
    "    proxyMetadata={\n",
    "        'name': pxy.name,\n",
    "        'description': pxy.description,\n",
    "        'file': os.path.basename(pxy.file),\n",
    "        'license': pxy.license.asDict(),\n",
    "        'vertexBoneWeights_file': pxy.vertexBoneWeights_file,\n",
    "        'version': pxy.version,\n",
    "        'obj_file': os.path.basename(pxy.obj_file),\n",
    "        'basemesh': pxy.basemesh,\n",
    "    }\n",
    "else:\n",
    "    proxyMetadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:39.671687Z",
     "start_time": "2017-04-18T08:57:39.665509+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available rigs:\n",
      "  data/rigs\\default.mhskel\n",
      "  data/rigs\\default_no_toes.mhskel\n",
      "  data/rigs\\game_engine.mhskel\n",
      "rig data/rigs/default_no_toes.mhskel\n"
     ]
    }
   ],
   "source": [
    "# choose rig\n",
    "rigfile='data/rigs/default_no_toes.mhskel'\n",
    "# some poses involve balled hands etc, so I choose this one with ~130 bones as opposed to the 30\n",
    "\n",
    "# list\n",
    "with mhpath:\n",
    "    paths = [getpath.getSysDataPath('rigs')]\n",
    "    files=getpath.search(paths, ['.mhskel'], False)\n",
    "    print \"Available rigs:\"\n",
    "    print \"\\n\".join(['  %s' % r for r in files])\n",
    "\n",
    "\n",
    "    rigfile = getpath.findFile(rigfile,\n",
    "                               searchPaths = [getpath.getSysDataPath(),\n",
    "                                              getpath.getSysPath()])\n",
    "    print 'rig', rigfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:39.685299Z",
     "start_time": "2017-04-18T08:57:39.672946+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): arrow in c:\\python27\\lib\\site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in c:\\python27\\lib\\site-packages (from arrow)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in c:\\python27\\lib\\site-packages (from python-dateutil->arrow)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 7.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('outputdir', 'D:\\\\Projects\\\\MakeHuman\\\\VAE_2.0\\\\notebooks\\\\output\\\\20170420-223432_None_vae_data')\n"
     ]
    }
   ],
   "source": [
    "# !pip install path\n",
    "!pip install arrow\n",
    "from path import Path\n",
    "import arrow\n",
    "import uuid\n",
    "\n",
    "# choose a new outdir\n",
    "os.chdir(basedir)\n",
    "outputdir = os.path.abspath(os.path.join(\"output\",runname+\"_vae_data\"))\n",
    "print ('outputdir',outputdir)\n",
    "try:\n",
    "    os.makedirs(outputdir)\n",
    "except Exception as exc:\n",
    "    print('makedir',exc)\n",
    "    pass\n",
    "\n",
    "outputdir = Path(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:41.821518Z",
     "start_time": "2017-04-18T08:57:39.686435+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some consistent scheme for modifier values (alphebetical order) \n",
    "# and scale them to 0-1\n",
    "# is_metadetail = np.array(['macrodetails' in s for s in target_dict.values()])*1\n",
    "from collections import OrderedDict\n",
    "human = getHuman()\n",
    "target_dict = OrderedDict(enumerate(sorted([m.fullName for m in human.modifiers])))\n",
    "target_dict_rev = OrderedDict((v,k) for k,v in target_dict.items())\n",
    "\n",
    "mins = np.array([i[1] for i in sorted([(m.fullName,m.getMin()) for m in human.modifiers])])\n",
    "maxs = np.array([i[1] for i in sorted([(m.fullName,m.getMax()) for m in human.modifiers])])\n",
    "\n",
    "def modifiers2params(modifierValues):\n",
    "    \"\"\"Shift the params from [-1,1] to [0,1]\"\"\"\n",
    "    v=np.array([modifierValues[m] for m in target_dict_rev.keys()])\n",
    "    return (v-mins)/(maxs-mins)\n",
    "\n",
    "\n",
    "def params2modifiers(params):\n",
    "    r=params*(maxs-mins)+mins\n",
    "    # shuffle the order like in the random plugin for makehuman\n",
    "    i=zip(target_dict_rev.keys(),r)\n",
    "    np.random.shuffle(i)\n",
    "    return OrderedDict(i)\n",
    "    \n",
    "def rand_modifier_values():\n",
    "    r=np.random.random((len(human.modifiers)))\n",
    "    return params2modifiers(r)\n",
    "\n",
    "modifier_values=rand_modifier_values()\n",
    "mv=np.array(modifier_values.values())\n",
    "# test it started from [-1,1]\n",
    "assert (mv<=0).any()\n",
    "assert (mv>=-1).all()\n",
    "p=modifiers2params(modifier_values)\n",
    "# test it's moved to [0,1]\n",
    "assert (p>=0).all()\n",
    "assert (p<=1).all()\n",
    "# check it's invertible\n",
    "assert (modifiers2params(params2modifiers(p))==p).all(), 'should be invertible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:41.830238Z",
     "start_time": "2017-04-18T08:57:41.823084+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EthnicModifier macrodetails/African, 0.0, 0.3333333333333333, 1.0)\n",
      "(UniversalModifier head/head-invertedtriangular, 0.0, 0, 1.0)\n",
      "(EthnicModifier macrodetails/Asian, 0.0, 0.3333333333333333, 1.0)\n",
      "(UniversalModifier head/head-oval, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-diamond, 0.0, 0, 1.0)\n",
      "(EthnicModifier macrodetails/Caucasian, 0.0, 0.3333333333333333, 1.0)\n",
      "(UniversalModifier head/head-triangular, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-square, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-round, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-rectangular, 0.0, 0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# make sure defaults are right\n",
    "modifier_values = params2modifiers(np.ones_like(p)/2)\n",
    "for m in human.modifiers:\n",
    "    v = modifier_values[m.fullName]\n",
    "    assert v<=m.getMax()\n",
    "    assert v>=m.getMin()\n",
    "    try:\n",
    "        assert v==m.getDefaultValue(),'%s'%m\n",
    "    except:\n",
    "        print(m,m.getMin(),m.getDefaultValue(),m.getMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:41.916495Z",
     "start_time": "2017-04-18T08:57:41.831517+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\MakeHuman\\VAE_2.0\\notebooks\\output\\20170420-223432_None_vae_data\\metadata.json\n"
     ]
    }
   ],
   "source": [
    "# write metadata\n",
    "target_dict_file = outputdir.joinpath('metadata.json')\n",
    "\n",
    "metadata = {\n",
    "    \"name\"      : runname,\n",
    "    \"nvertex\"   : basehuman.mesh.coord.shape,\n",
    "    \"proxyMetadata\": json.dumps(proxyMetadata),\n",
    "    \"target_dict\": target_dict,\n",
    "    \"date\": arrow.utcnow().format()\n",
    "}\n",
    "\n",
    "json.dump(metadata, open(target_dict_file,'w'))\n",
    "print(target_dict_file)\n",
    "metadata\n",
    "\n",
    "# define x and y files\n",
    "X_file = outputdir.joinpath('X_train.hdf5')\n",
    "y_file = outputdir.joinpath('y_train.hdf5')\n",
    "\n",
    "# write headers\n",
    "import tables\n",
    "with tables.open_file(X_file, 'w') as xfo:\n",
    "    data = basehuman.mesh.coord\n",
    "    atom = tables.Atom.from_dtype(data.dtype)\n",
    "    # create an expandable array\n",
    "    data_storage = xfo.create_earray(xfo.root, 'data', atom, (0,data.shape[0],data.shape[1]))\n",
    "\n",
    "# process the results\n",
    "values = dict([(m.fullName,m.getValue()) for m in basehuman.modifiers])\n",
    "data = np.array([values.get(t,0) for t in target_dict.values()],dtype=np.float32)\n",
    "data\n",
    "with tables.open_file(y_file, 'w') as xfo:\n",
    "    atom = tables.Atom.from_dtype(data.dtype)\n",
    "    # create an expandable array\n",
    "    data_storage = xfo.create_earray(xfo.root, 'data', atom, (0,data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:45.942546Z",
     "start_time": "2017-04-18T08:57:41.917682+08:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make base human model\n",
    "with mhpath: \n",
    "    basehuman = getHuman()\n",
    "    \n",
    "    # rig\n",
    "    if rigfile:\n",
    "        humanargparser.addRig(basehuman,rigfile)\n",
    "\n",
    "    if proxyname:\n",
    "        # load proxy\n",
    "        pxy = proxy.loadTextProxy(basehuman, proxyfile, type='Proxymesh')\n",
    "        mesh,obj = pxy.loadMeshAndObject(basehuman)\n",
    "        basehuman.setProxy(pxy)\n",
    "\n",
    "        # _adaptProxyToHuman\n",
    "        mesh = obj.getSeedMesh() #  just gets proxy mesh\n",
    "        pxy.update(mesh) # mesh change cords and calc normals\n",
    "        mesh.update() # calc norms, coorgs and tangents\n",
    "    \n",
    "    randomValues=rand_modifier_values()\n",
    "    params=modifiers2params(randomValues)\n",
    "    params = np.ones_like(params)/2 # grab middle values\n",
    "    randomValues=params2modifiers(params)\n",
    "    human.updateMacroModifiers()\n",
    "    basehuman = assignModifierValues(basehuman, randomValues)\n",
    "    \n",
    "    mesh = basehuman.meshData\n",
    "    group_mask = np.ones(len(mesh._faceGroups), dtype=bool)\n",
    "    face_mask = group_mask[mesh.group]\n",
    "    basehuman._staticFaceMask = face_mask\n",
    "    basehuman.meshData.changeFaceMask(basehuman.staticFaceMask)\n",
    "    basehuman.meshData.updateIndexBufferFaces()\n",
    "    basehuman.changeVertexMask(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:49.785317Z",
     "start_time": "2017-04-18T08:57:45.944309+08:00"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 998/998 [27:04<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "# make the edge cases mixed with some random ones\n",
    "nb_targets=len(target_dict)\n",
    "edge_params = np.ones((2*nb_targets+1,nb_targets))/2\n",
    "for i in range(nb_targets):\n",
    "    edge_params[i,i]=1.0\n",
    "    edge_params[nb_targets+i,i]=0.0\n",
    "    \n",
    "# add some random ones and shuffle\n",
    "edge_params = np.concatenate([edge_params,np.random.random(edge_params.shape)])\n",
    "np.random.shuffle(edge_params)\n",
    "\n",
    "for params in tqdm(edge_params, leave=True):\n",
    "    # fresh args but re-use the old human to save time initialising\n",
    "    with mhpath: \n",
    "\n",
    "        human = getHuman()\n",
    "\n",
    "        if proxyname:\n",
    "            # load proxy\n",
    "            pxy = proxy.loadTextProxy(human, proxyfile, type='Proxymesh')\n",
    "            mesh,obj = pxy.loadMeshAndObject(human)\n",
    "            human.setProxy(pxy)\n",
    "\n",
    "            # _adaptProxyToHuman\n",
    "            mesh = obj.getSeedMesh() #  just gets proxy mesh\n",
    "            pxy.update(mesh) # mesh change cords and calc normals\n",
    "            mesh.update() # calc norms, coorgs and tangents\n",
    "\n",
    "        randomValues = params2modifiers(params)\n",
    "        human = assignModifierValues(human, randomValues)\n",
    "        morphTarget = -(basehuman.mesh.coord-human.mesh.coord)       \n",
    "        \n",
    "        \n",
    "        # append the results to a file as float32\n",
    "        with tables.open_file(X_file, 'a') as xfo:\n",
    "            xfo.root.data.append([morphTarget])\n",
    "        with tables.open_file(y_file, 'a') as xfo:\n",
    "            xfo.root.data.append([params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:52.301785Z",
     "start_time": "2017-04-18T08:57:52.255818+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rv=np.array([m[1] for m in sorted([(m.fullName,m.getValue()) for m in human.modifiers])])\n",
    "rv2=np.array([m[1] for m in sorted(randomValues.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:52.472037Z",
     "start_time": "2017-04-18T08:57:52.463454+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(human.targetsDetailStack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:57:59.160414Z",
     "start_time": "2017-04-18T08:57:52.869275+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16000/16000 [7:28:20<00:00,  1.64s/it]     \n"
     ]
    }
   ],
   "source": [
    "# now make random targets and save to file\n",
    "for t in tqdm(range(16000), leave=True):\n",
    "    # fresh args but re-use the old human to save time initialising\n",
    "    with mhpath: \n",
    "\n",
    "        human = getHuman()\n",
    "\n",
    "        if proxyname:\n",
    "            # load proxy\n",
    "            pxy = proxy.loadTextProxy(human, proxyfile, type='Proxymesh')\n",
    "            mesh,obj = pxy.loadMeshAndObject(human)\n",
    "            human.setProxy(pxy)\n",
    "\n",
    "            # _adaptProxyToHuman\n",
    "            mesh = obj.getSeedMesh() #  just gets proxy mesh\n",
    "            pxy.update(mesh) # mesh change cords and calc normals\n",
    "            mesh.update() # calc norms, coorgs and tangents\n",
    "\n",
    "        # set random targets\n",
    "        randomValues=rand_modifier_values()\n",
    "        params=modifiers2params(randomValues)\n",
    "        human = assignModifierValues(human, randomValues)\n",
    "        \n",
    "        # process the results\n",
    "#         values = dict([(m.fullName,m.getValue()) for m in human.modifiers])\n",
    "#         params = np.array([values.get(t,0) for t in target_dict.values()],dtype=np.float32)\n",
    "        # normalise the params? shift to 0-1 except for metadata ones which already are\n",
    "#         params = normalise_params(params)\n",
    "        morphTarget = -(basehuman.mesh.coord-human.mesh.coord)       \n",
    "        \n",
    "        \n",
    "        # append the results to a file as float32\n",
    "        with tables.open_file(X_file, 'a') as xfo:\n",
    "            xfo.root.data.append([morphTarget])\n",
    "        with tables.open_file(y_file, 'a') as xfo:\n",
    "            xfo.root.data.append([params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:58:01.660906Z",
     "start_time": "2017-04-18T08:58:01.606912+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export SKELETON\n",
    "skeleton=basehuman.getSkeleton()\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "import transformations as tm\n",
    "\n",
    "## add to metadata\n",
    "jsondata = OrderedDict()\n",
    "if \"metadata\" not in jsondata:\n",
    "    jsondata[\"metadata\"]={}\n",
    "skeletonMetadata=jsondata[\"metadata\"][\"skeleton\"]={ \"name\": skeleton.name,\n",
    "                         \"version\": skeleton.version,\n",
    "                         \"description\": skeleton.description,\n",
    "                         \"plane_map_strategy\": skeleton.plane_map_strategy,\n",
    "                         \"license\": skeleton.license.asDict(),\n",
    "                       }\n",
    "\n",
    "lastPos=skeleton.getBones()[0].headPos\n",
    "# bones\n",
    "bones = []\n",
    "for bone in skeleton.getBones():\n",
    "    bonedef = {}\n",
    "    bonedef[\"name\"]=bone.name\n",
    "    \n",
    "    if bone.parent:\n",
    "        bonedef[\"parent\"] = bone.parent.index\n",
    "    else:\n",
    "        bonedef[\"parent\"] = -1\n",
    "    \n",
    "    # distortion form res, [0,0,0,1] is null in threejs\n",
    "    bonedef[\"rotq\"] = [0, 0, 0, 1]\n",
    "    \n",
    "    # tail position relative to parent\n",
    "    if bone.parent:\n",
    "        bonedef[\"pos\"]= (bone.tailPos-bone.parent.tailPos).tolist()\n",
    "    else:\n",
    "        bonedef[\"pos\"]= bone.tailPos.tolist()\n",
    "        # adjust for offset in center of geom? nope\n",
    "        # bonedef[\"pos\"][1]-=human.getBoundingBox().mean(0)[1]\n",
    "    \n",
    "    # scale\n",
    "    bonedef[\"scl\"]= [skeleton.scale,skeleton.scale,skeleton.scale] \n",
    "\n",
    "    bones.append(bonedef)\n",
    "jsondata[\"bones\"] = bones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:58:05.212679Z",
     "start_time": "2017-04-18T08:58:05.207155+08:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "world_tail_pos={}\n",
    "for bone in bones:\n",
    "    pos=np.array(bone[\"pos\"])\n",
    "    if bone[\"parent\"]!=-1:\n",
    "        parent = bones[bone[\"parent\"]]\n",
    "        pos+=world_tail_pos[parent['name']]\n",
    "    world_tail_pos[bone['name']]=pos\n",
    "df=pd.DataFrame.from_dict(world_tail_pos,orient=\"index\")\n",
    "df.columns=['x','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T08:40:26.631517",
     "start_time": "2016-11-16T08:40:26.605659"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:58:05.889691Z",
     "start_time": "2017-04-18T08:58:05.695246+08:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save weights and inds\n",
    "# the data is indexes by bones but we need to reorder it by vertex, so first compile the data\n",
    "\n",
    "# get weights for the proxied mesh\n",
    "if proxyname:\n",
    "    vertex_weights=basehuman.mesh.getVertexWeights(basehuman.proxy.getVertexWeights(skeleton.getVertexWeights()))\n",
    "else:\n",
    "    vertex_weights=basehuman.mesh.getVertexWeights(skeleton.getVertexWeights())\n",
    "\n",
    "from collections import defaultdict\n",
    "vwdata=defaultdict(dict)\n",
    "for bone in skeleton.getBones():\n",
    "    if bone.name in vertex_weights.data:\n",
    "        for vert,w in zip(*vertex_weights.data[bone.name]):\n",
    "            vwdata[vert][bone.index]=w\n",
    "\n",
    "\n",
    "# now we can export them\n",
    "LOST_CONNECTIONS=0\n",
    "\n",
    "jsondata[\"influencesPerVertex\"] = influencesPerVertex = 4 # max unless I change skinnedMesh\n",
    "skinIndices=[]\n",
    "skinWeights=[]\n",
    "for vert in range(vertex_weights.vertexCount):\n",
    "    vw=vwdata[vert].items()\n",
    "    a=np.array(vw).T\n",
    "    a.sort(-1) # sort by weight\n",
    "    \n",
    "    if a.shape[1]>influencesPerVertex:\n",
    "        LOST_CONNECTIONS+=a.shape[1]-influencesPerVertex\n",
    "    \n",
    "    # pad and crop\n",
    "    pca=np.zeros((2,influencesPerVertex), dtype=float)\n",
    "    min_size=min(pca.shape[1],a.shape[1])\n",
    "    pca[:2,:min_size]=a[:2,:min_size]\n",
    "    \n",
    "    skinIndices+=pca[0,:].tolist()\n",
    "    skinWeights+=pca[1,:].tolist()\n",
    "jsondata[\"skinWeights\"] = skinWeights\n",
    "jsondata[\"skinIndices\"] = skinIndices\n",
    "\n",
    "assert len(skinWeights)==len(skinIndices),'Should have equal number of inds and weights'\n",
    "assert vertex_weights.vertexCount*influencesPerVertex==len(skinIndices),'should have 4 times as many inds as vertices'\n",
    "assert max(skinIndices)<vertex_weights.vertexCount, 'vertex index should not refer to more vertices than we have'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T08:34:25.149546",
     "start_time": "2016-11-16T08:34:25.147555"
    }
   },
   "source": [
    "# export base human while masking helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:49:46.370875Z",
     "start_time": "2017-04-18T08:49:46.367884+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:54:24.038865Z",
     "start_time": "2017-04-18T08:54:23.968222+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:58:07.755909Z",
     "start_time": "2017-04-18T08:58:07.353202+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Projects\\MakeHuman\\VAE_2.0\\vendor\\makehuman-commandline\\makehuman/data/3dobjs/base.obj\n"
     ]
    }
   ],
   "source": [
    "# combine base and morphs while giving helpers a diff index\n",
    "\n",
    "## TODO make this iterative\n",
    "\n",
    "# The base object has invisible groups but they are not parsed by json loader so I need to\n",
    "# change it to add materials for each group, and a matrial index. This will let me hide them in three js\n",
    "# at the moment they are just groups\n",
    "\n",
    "# inputs\n",
    "if proxyfile:\n",
    "    # use the proxy obj as it might have groups in which we want\n",
    "    infile=mhpath+'/'+proxyfile.replace('.proxy','.obj')\n",
    "else:\n",
    "    infile=mhpath+'/data/3dobjs/base.obj'\n",
    "#     infile=baseargs[\"outputs\"][0]\n",
    "print infile\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "morphfilesa=[]\n",
    "morphfilesa\n",
    "morphfiles=' '.join(morphfilesa)\n",
    "colorfiles=''\n",
    "outfile=os.path.abspath(os.path.join(\"output\",\"json\",runname+\"_mh2obh2json.json\"))\n",
    "\n",
    "# start by parsing base mh obj\n",
    "faces, vertices, uvs, normals, materials, mtllib = wrap_mh.convert.convert_obj_three.parse_obj(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:58:08.341029Z",
     "start_time": "2017-04-18T08:58:08.310861+08:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# modify the base obj\n",
    "#################################\n",
    "\n",
    "# order unique groups by how common they are\n",
    "groupCounts = Counter([f['group'] for f in faces])\n",
    "groups = [g[0] for g in groupCounts.most_common()]\n",
    "\n",
    "# set material index\n",
    "for i in range(len(faces)):\n",
    "    group = faces[i][\"group\"]\n",
    "    material = groups.index(group)\n",
    "    faces[i][\"material\"] = material\n",
    "\n",
    "# and make those materials\n",
    "materials={}\n",
    "for group in groups:\n",
    "    materialInd = groups.index(group)\n",
    "    materials[group]=materialInd\n",
    "#     {\n",
    "#         \"DbgColor\" : 15658734,\n",
    "#         \"DbgIndex\" : materialInd,\n",
    "#         \"DbgName\" : group,\n",
    "#         \"colorDiffuse\" : [0.5903, 0.44, 0.338],\n",
    "#         \"colorSpecular\" : [0.3, 0.3, 0.3],\n",
    "#         \"opacity\" : group!=\"body\"\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:58:09.188778Z",
     "start_time": "2017-04-18T08:58:09.151871+08:00"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# now regular loading from parse_ascii\n",
    "#########################################3\n",
    "n_vertices = len(vertices)\n",
    "n_faces = len(faces)\n",
    "\n",
    "# align model\n",
    "if ALIGN == \"center\":\n",
    "    center(vertices)\n",
    "elif ALIGN == \"centerxz\":\n",
    "    centerxz(vertices)\n",
    "elif ALIGN == \"bottom\":\n",
    "    bottom(vertices)\n",
    "elif ALIGN == \"top\":\n",
    "    top(vertices)\n",
    "\n",
    "# generate normals string\n",
    "nnormal = 0\n",
    "normals_string = \"\"\n",
    "if SHADING == \"smooth\":\n",
    "    normals_string = \",\".join(generate_normal(n) for n in normals)\n",
    "    nnormal = len(normals)\n",
    "\n",
    "# extract face groups\n",
    "fgs = [f['group'] for f in faces]\n",
    "groups = list(set(fgs))\n",
    "faceGroups = [groups.index(g) for g in fgs]\n",
    "\n",
    "# extract morph vertices\n",
    "# chance of MEMMORY ERROR here. Might need to append this to the file part by art. Or load morph seperatley\n",
    "morphTargets = generate_morph_targets(morphfiles, n_vertices, infile)\n",
    "\n",
    "# do a test to check that morphtargets are diff here\n",
    "\n",
    "# extract morph colors\n",
    "morphColors, colorFaces, materialColors = generate_morph_colors(colorfiles, n_vertices, n_faces)\n",
    "\n",
    "# TODO\n",
    "animations=[]\n",
    "\n",
    "# generate colors string\n",
    "\n",
    "ncolor = 0\n",
    "colors_string = \"\"\n",
    "\n",
    "if len(colorFaces) < len(faces):\n",
    "    colorFaces = faces\n",
    "    materialColors = extract_material_colors(materials, mtllib, infile)\n",
    "\n",
    "if BAKE_COLORS:\n",
    "    colors_string = \",\".join(generate_color_decimal(c) for c in materialColors)\n",
    "    ncolor = len(materialColors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T01:00:01.251143Z",
     "start_time": "2017-04-18T09:00:00.779628+08:00"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19158 vertices, 18486 faces, 139 materials, 0 morph targets. To D:\\Projects\\MakeHuman\\VAE_2.0\\notebooks\\output\\20170420-223432_None_vae_data\\basemodel.json\n"
     ]
    }
   ],
   "source": [
    "# transform morph targets names from data_targets_asym_asym-eye-5-l.target => data/targets/asym/asym-eye-5-l.target being wary of doulbe underscores we don't want to replace\n",
    "# since we need the slashes in the end product but can't have them in the interim product else they would be interpreted as dirs\n",
    "# and stripped. TODO proboboly best to seperate target paths and names anyway\n",
    "morphTargets = [m.replace('__','+').replace('_','/').replace('+','_') for m in morphTargets]\n",
    "\n",
    "\n",
    "# now make dict to export as json\n",
    "\n",
    "\n",
    "TEMPLATE_FILE_ASCII = u\"\"\"{\n",
    "\n",
    "    \"metadata\" :\n",
    "    {\n",
    "        \"name\"          : \"%(name)s\",\n",
    "        \"formatVersion\" : 3.1,\n",
    "        \"sourceFile\"    : \"%(fname)s\",\n",
    "        \"generatedBy\"   : \"OBJConverter\",\n",
    "        \"vertices\"      : %(nvertex)d,\n",
    "        \"faces\"         : %(nface)d,\n",
    "        \"normals\"       : %(nnormal)d,\n",
    "        \"colors\"        : %(ncolor)d,\n",
    "        \"uvs\"           : %(nuv)d,\n",
    "        \"materials\"     : %(nmaterial)d,\n",
    "        \"morphTargets\"  : %(nmorphs)d,\n",
    "        \"bones\"         : %(nbones)d,\n",
    "        \"license\"       : %(license)s,\n",
    "        \"skeleton\"      : %(skeletonMetadata)s,\n",
    "        \"proxymesh\"     : %(proxyMetadata)s\n",
    "    },\n",
    "\n",
    "    \"scale\" : %(scale)f,\n",
    "\n",
    "    \"materials\": [%(materials)s],\n",
    "\n",
    "    \"vertices\": [%(vertices)s],\n",
    "\n",
    "    \"morphTargets\": [%(morphTargets)s],\n",
    "\n",
    "    \"morphColors\": [%(morphColors)s],\n",
    "\n",
    "    \"normals\": [%(normals)s],\n",
    "\n",
    "    \"colors\": [%(colors)s],\n",
    "\n",
    "    \"uvs\": [[%(uvs)s]],\n",
    "\n",
    "    \"faces\": [%(faces)s],\n",
    "\n",
    "    \"bones\" : %(bones)s,\n",
    "    \n",
    "    \"skinWeights\": %(skinWeights)s,\n",
    "    \n",
    "    \"skinIndices\": %(skinIndices)s,\n",
    "    \n",
    "    \"influencesPerVertex\": %(influencesPerVertex)d,\n",
    "\n",
    "    \"animations\" : [%(animations)s]\n",
    "\n",
    "}\"\"\"\n",
    "text =  TEMPLATE_FILE_ASCII % {\n",
    "    \"name\"      : get_name(outfile),\n",
    "    \"fname\"     : os.path.basename(infile),\n",
    "    \"nvertex\"   : len(vertices),\n",
    "    \"nface\"     : len(faces),\n",
    "    \"nuv\"       : len(uvs),\n",
    "    \"nnormal\"   : nnormal,\n",
    "    \"ncolor\"    : ncolor,\n",
    "    \"nmaterial\" : len(materials),\n",
    "    \"nmorphs\":len(morphTargets),\n",
    "    \"nbones\": len(bones),\n",
    "    \"license\": json.dumps(mh_licence.asDict()),\n",
    "    \"skeletonMetadata\": json.dumps(skeletonMetadata),\n",
    "    \"proxyMetadata\": json.dumps(proxyMetadata),\n",
    "\n",
    "    \"materials\" : generate_materials_string(materials, mtllib, infile),\n",
    "\n",
    "    \"normals\"       : normals_string,\n",
    "    \"colors\"        : colors_string,\n",
    "    \"uvs\"           : \",\".join(generate_uv(uv) for uv in uvs),\n",
    "    \"vertices\"      : \",\".join(generate_vertex(v, TRUNCATE, SCALE) for v in vertices),\n",
    "\n",
    "    \"morphTargets\"  : \"\\n%s\\n\\t\" % \",\\n\".join(morphTargets),\n",
    "    \"morphColors\"   : morphColors,\n",
    "\n",
    "    \"faces\"     : \",\".join(generate_face(f, fc) for f, fc in zip(faces, colorFaces)),\n",
    "\n",
    "    \"scale\"    : SCALE,\n",
    "\n",
    "    \"bones\": json.dumps(bones),\n",
    "\n",
    "    \"animations\": \",\".join(animations),\n",
    "    \n",
    "    \"skinWeights\": json.dumps(skinWeights),\n",
    "    \"skinIndices\": json.dumps(skinIndices),\n",
    "    \"influencesPerVertex\":influencesPerVertex\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "baseoutfile=outputdir.joinpath('basemodel.json')\n",
    "out = open(baseoutfile, \"w\")\n",
    "out.write(text)\n",
    "out.close()\n",
    "\n",
    "s=json.dumps(jsondata,indent=4)\n",
    "json.loads(s)\n",
    "\n",
    "print \"%d vertices, %d faces, %d materials, %d morph targets. To %s\" % (len(vertices), len(faces), len(materials), len(morphTargets), baseoutfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-04-18T00:59:59.669401Z",
     "start_time": "2017-04-18T08:59:59.667279+08:00"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6.0,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}