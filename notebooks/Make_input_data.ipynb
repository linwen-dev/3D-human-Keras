{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:34:03.020844",
     "start_time": "2016-11-17T19:33:59.696129"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from path import Path\n",
    "import json\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "basedir=r'/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/'\n",
    "os.chdir(basedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:34:39.859182",
     "start_time": "2016-11-17T19:34:39.856508"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from path import Path\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import makehuman-cmd\n",
    "\n",
    "https://bitbucket.org/duststorm01/makehuman-commandline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:33:52.383037",
     "start_time": "2016-11-17T19:33:48.185469"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized logging\n",
      "Loading binary mesh data/3dobjs/base.npz.\n",
      "Loading material from file data/skins/default.mhmat\n",
      "Loading binary mesh data/3dobjs/base.npz.\n",
      "Loading material from file data/skins/default.mhmat\n",
      "Loading modifiers from data/modifiers/modeling_modifiers.json\n",
      "Attempting to load targets from NPZ file.\n",
      "1258 targets loaded from NPZ file succesfully.\n",
      "Loaded 249 modifiers from file data/modifiers/modeling_modifiers.json\n",
      "Loaded 198 modifier descriptions from file data/modifiers/modeling_modifiers_desc.json\n"
     ]
    }
   ],
   "source": [
    "# from .mh_helpers import clean, short_hash, clean_modifier\n",
    "\n",
    "mhpath = Path(os.path.abspath(\"../vendor/makehuman-commandline/makehuman\"))\n",
    "\n",
    "#===============================================================================\n",
    "# Import Makehuman resources, needs to be with makehuman dir as current dir\n",
    "#===============================================================================\n",
    "\n",
    "appcwd = os.path.abspath(os.curdir)\n",
    "sys.path.append(mhpath)\n",
    "sys.path.append(appcwd)\n",
    "sys.path.append('.')\n",
    "\n",
    "def getHuman():\n",
    "    \"\"\"Load a human model with modifiers.\"\"\"\n",
    "    with mhpath:\n",
    "        # maxFaces *uint* Number of faces per vertex (pole), None for default (min 4)\n",
    "        human = Human(files3d.loadMesh(\n",
    "            getpath.getSysDataPath(\"3dobjs/base.obj\"),\n",
    "            maxFaces=5))\n",
    "        # load modifiers onto human\n",
    "        humanmodifier.mods_loaded = False\n",
    "        modifiers = humanmodifier.loadModifiers(\n",
    "            getpath.getSysDataPath('modifiers/modeling_modifiers.json'), human)\n",
    "        return human\n",
    "\n",
    "with mhpath:\n",
    "    import makehuman\n",
    "    oldpath = os.sys.path\n",
    "    makehuman.set_sys_path()\n",
    "    # make makehuman paths absolute by going through newest paths and making abs\n",
    "    for i in range(len(os.sys.path)):\n",
    "        p = os.sys.path[i]\n",
    "        if p[0:2] == './':\n",
    "            os.sys.path[i] = os.path.join(\n",
    "                os.path.abspath('.'), p.replace('./', ''))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    makehuman.init_logging()\n",
    "    #import image_pil as image_lib\n",
    "    #\n",
    "    import proxy as mhproxy\n",
    "    import humanargparser\n",
    "    import targets as mhtargets\n",
    "    from human import Human\n",
    "    import files3d\n",
    "    import getpath\n",
    "    import humanmodifier\n",
    "    from core import G\n",
    "    import headless\n",
    "    import autoskinblender\n",
    "    import export\n",
    "    \n",
    "    # Init console app\n",
    "    with mhpath:\n",
    "        G.app = headless.ConsoleApp()\n",
    "    G.app.selectedHuman = human = getHuman()\n",
    "    headless.OBJExporter = None\n",
    "    headless.MHXExporter = None\n",
    "    headless.MhxConfig = None\n",
    "    humanargparser.mods_loaded = False\n",
    "\n",
    "def assignModifierValues(human, valuesDict):\n",
    "    _tmp = human.symmetryModeEnabled\n",
    "    human.symmetryModeEnabled = False\n",
    "    for mName, val in valuesDict.items():\n",
    "        try:\n",
    "            human.getModifier(mName).setValue(val)\n",
    "        except:\n",
    "            pass\n",
    "    human.updateMacroModifiers()\n",
    "    human.applyAllTargets()\n",
    "    human.symmetryModeEnabled = _tmp\n",
    "    return human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:34:44.339654",
     "start_time": "2016-11-17T19:34:44.262698"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import proxies\n",
    "def _listDataFiles(foldername,\n",
    "                   extensions,\n",
    "                   onlySysData=False,\n",
    "                   recursive=True):\n",
    "    with mhpath:  # sadly makehuman seems hardcoded\n",
    "        if onlySysData:\n",
    "            paths = [getpath.getSysDataPath(foldername)]\n",
    "        else:\n",
    "            paths = [getpath.getDataPath(foldername),\n",
    "                     getpath.getSysDataPath(foldername)]\n",
    "    return list(getpath.search(paths, extensions, recursive))\n",
    "\n",
    "def clean(s):\n",
    "    \"\"\"Remove invalid characters.\"\"\"\n",
    "    s = re.sub('[^0-9a-zA-Z_]', '_', s)\n",
    "    return s\n",
    "\n",
    "with mhpath:\n",
    "    mhproxy.ProxyTypes\n",
    "    proxies = OrderedDict()\n",
    "    for proxyType in mhproxy.ProxyTypes:\n",
    "        files = list(_listDataFiles(proxyType.lower(),\n",
    "                                         ['.proxy', '.mhclo']))\n",
    "        for f in files:\n",
    "            if proxyType not in proxies.keys():\n",
    "                proxies[proxyType] = OrderedDict()\n",
    "            filesname = clean(os.path.splitext(os.path.basename(f))[0])\n",
    "            proxies[proxyType][filesname] = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:19.320655",
     "start_time": "2016-11-17T19:35:19.315792"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# old dupicate imports\n",
    "import wrap_mh\n",
    "import proxy\n",
    "from human import Human\n",
    "import getpath\n",
    "from pprint import pprint\n",
    "import files3d\n",
    "import humanargparser\n",
    "import humanmodifier\n",
    "\n",
    "mhpath=wrap_mh.mhpath\n",
    "\n",
    "\n",
    "# add mh lic info\n",
    "from makehuman import LicenseInfo\n",
    "mh_licence=LicenseInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:24.271245",
     "start_time": "2016-11-17T19:35:24.268610"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# silence mh\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make random outputs for a VAE\n",
    "\n",
    "TODO\n",
    "- tidy\n",
    "- I'm saving -morphTargets, so flip it around next time I generate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export all modifiers as obj's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:30.294973",
     "start_time": "2016-11-17T19:35:27.507974"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxies to choose\n",
      "[u'female1605',\n",
      " u'female_generic',\n",
      " u'female_muscle_13442',\n",
      " u'male1591',\n",
      " u'male_generic',\n",
      " u'male_muscle_13290',\n",
      " u'proxy10k',\n",
      " u'human',\n",
      " u'proxy5k',\n",
      " u'proxy741',\n",
      " u'proxymesh_test']\n",
      "runname 20161117-193527_None\n"
     ]
    }
   ],
   "source": [
    "# choose proxymesh\n",
    "proxyname= None #'female1605'\n",
    "print 'Proxies to choose'\n",
    "pprint(proxies['Proxymeshes'].keys())\n",
    "export_helpers=False\n",
    "\n",
    "if proxyname:\n",
    "    proxyfile=proxies['Proxymeshes'][proxyname]\n",
    "else:\n",
    "    proxyfile=None\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "runname=timestr+'_'+str(proxyname)\n",
    "print 'runname',runname\n",
    "\n",
    "basehuman=getHuman()\n",
    "with mhpath:\n",
    "    if proxyname:\n",
    "        pxy = proxy.loadTextProxy(basehuman, proxyfile, type='Proxymesh')\n",
    "    else:\n",
    "        pxy = None\n",
    "if proxyname:\n",
    "    proxyMetadata={\n",
    "        'name': pxy.name,\n",
    "        'description': pxy.description,\n",
    "        'file': os.path.basename(pxy.file),\n",
    "        'license': pxy.license.asDict(),\n",
    "        'vertexBoneWeights_file': pxy.vertexBoneWeights_file,\n",
    "        'version': pxy.version,\n",
    "        'obj_file': os.path.basename(pxy.obj_file),\n",
    "        'basemesh': pxy.basemesh,\n",
    "    }\n",
    "else:\n",
    "    proxyMetadata = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:33.719335",
     "start_time": "2016-11-17T19:35:33.707787"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available rigs:\n",
      "  data/rigs/cmu_mb.mhskel\n",
      "  data/rigs/default.mhskel\n",
      "  data/rigs/default_no_toes.mhskel\n",
      "  data/rigs/game_engine.mhskel\n",
      "rig data/rigs/default_no_toes.mhskel\n"
     ]
    }
   ],
   "source": [
    "# choose rig\n",
    "rigfile='data/rigs/default_no_toes.mhskel'\n",
    "# some poses involve balled hands etc, so I choose this one with ~130 bones as opposed to the 30\n",
    "\n",
    "# list\n",
    "with mhpath:\n",
    "    paths = [getpath.getSysDataPath('rigs')]\n",
    "    files=getpath.search(paths, ['.mhskel'], False)\n",
    "    print \"Available rigs:\"\n",
    "    print \"\\n\".join(['  %s' % r for r in files])\n",
    "\n",
    "\n",
    "    rigfile = getpath.findFile(rigfile,\n",
    "                               searchPaths = [getpath.getSysDataPath(),\n",
    "                                              getpath.getSysPath()])\n",
    "    print 'rig', rigfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:35.219825",
     "start_time": "2016-11-17T19:35:35.158835"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('outputdir', '/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/output/20161117-193527_None_vae_data')\n"
     ]
    }
   ],
   "source": [
    "from path import Path\n",
    "import arrow\n",
    "import uuid\n",
    "\n",
    "# choose a new outdir\n",
    "os.chdir(basedir)\n",
    "outputdir = os.path.abspath(os.path.join(\"output\",runname+\"_vae_data\"))\n",
    "print ('outputdir',outputdir)\n",
    "try:\n",
    "    os.makedirs(outputdir)\n",
    "except Exception as exc:\n",
    "    print('makedir',exc)\n",
    "    pass\n",
    "\n",
    "outputdir = Path(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:43.190627",
     "start_time": "2016-11-17T19:35:40.459773"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some consistent scheme for modifier values (alphebetical order) \n",
    "# and scale them to 0-1\n",
    "# is_metadetail = np.array(['macrodetails' in s for s in target_dict.values()])*1\n",
    "from collections import OrderedDict\n",
    "human = getHuman()\n",
    "target_dict = OrderedDict(enumerate(sorted([m.fullName for m in human.modifiers])))\n",
    "target_dict_rev = OrderedDict((v,k) for k,v in target_dict.items())\n",
    "\n",
    "mins = np.array([i[1] for i in sorted([(m.fullName,m.getMin()) for m in human.modifiers])])\n",
    "maxs = np.array([i[1] for i in sorted([(m.fullName,m.getMax()) for m in human.modifiers])])\n",
    "\n",
    "def modifiers2params(modifierValues):\n",
    "    \"\"\"Shift the params from [-1,1] to [0,1]\"\"\"\n",
    "    v=np.array([modifierValues[m] for m in target_dict_rev.keys()])\n",
    "    return (v-mins)/(maxs-mins)\n",
    "\n",
    "\n",
    "def params2modifiers(params):\n",
    "    r=params*(maxs-mins)+mins\n",
    "    # shuffle the order like in the random plugin for makehuman\n",
    "    i=zip(target_dict_rev.keys(),r)\n",
    "    np.random.shuffle(i)\n",
    "    return OrderedDict(i)\n",
    "    \n",
    "def rand_modifier_values():\n",
    "    r=np.random.random((len(human.modifiers)))\n",
    "    return params2modifiers(r)\n",
    "\n",
    "modifier_values=rand_modifier_values()\n",
    "mv=np.array(modifier_values.values())\n",
    "# test it started from [-1,1]\n",
    "assert (mv<=0).any()\n",
    "assert (mv>=-1).all()\n",
    "p=modifiers2params(modifier_values)\n",
    "# test it's moved to [0,1]\n",
    "assert (p>=0).all()\n",
    "assert (p<=1).all()\n",
    "# check it's invertible\n",
    "assert (modifiers2params(params2modifiers(p))==p).all(), 'should be invertible'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:43.198871",
     "start_time": "2016-11-17T19:35:43.192152"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(EthnicModifier macrodetails/African, 0.0, 0.3333333333333333, 1.0)\n",
      "(UniversalModifier head/head-invertedtriangular, 0.0, 0, 1.0)\n",
      "(EthnicModifier macrodetails/Caucasian, 0.0, 0.3333333333333333, 1.0)\n",
      "(UniversalModifier head/head-oval, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-diamond, 0.0, 0, 1.0)\n",
      "(EthnicModifier macrodetails/Asian, 0.0, 0.3333333333333333, 1.0)\n",
      "(UniversalModifier head/head-triangular, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-square, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-round, 0.0, 0, 1.0)\n",
      "(UniversalModifier head/head-rectangular, 0.0, 0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# make sure defaults are right\n",
    "modifier_values = params2modifiers(np.ones_like(p)/2)\n",
    "for m in human.modifiers:\n",
    "    v = modifier_values[m.fullName]\n",
    "    assert v<=m.getMax()\n",
    "    assert v>=m.getMin()\n",
    "    try:\n",
    "        assert v==m.getDefaultValue(),'%s'%m\n",
    "    except:\n",
    "        print(m,m.getMin(),m.getDefaultValue(),m.getMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:48.449436",
     "start_time": "2016-11-17T19:35:48.316815"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/output/20161117-193527_None_vae_data/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# write metadata\n",
    "target_dict_file = outputdir.joinpath('metadata.json')\n",
    "\n",
    "metadata = {\n",
    "    \"name\"      : runname,\n",
    "    \"nvertex\"   : basehuman.mesh.coord.shape,\n",
    "    \"proxyMetadata\": json.dumps(proxyMetadata),\n",
    "    \"target_dict\": target_dict,\n",
    "    \"date\": arrow.utcnow().format()\n",
    "}\n",
    "\n",
    "json.dump(metadata, open(target_dict_file,'w'))\n",
    "print(target_dict_file)\n",
    "metadata\n",
    "\n",
    "# define x and y files\n",
    "X_file = outputdir.joinpath('X_train.hdf5')\n",
    "y_file = outputdir.joinpath('y_train.hdf5')\n",
    "\n",
    "# write headers\n",
    "import tables\n",
    "with tables.open_file(X_file, 'w') as xfo:\n",
    "    data = basehuman.mesh.coord\n",
    "    atom = tables.Atom.from_dtype(data.dtype)\n",
    "    # create an expandable array\n",
    "    data_storage = xfo.create_earray(xfo.root, 'data', atom, (0,data.shape[0],data.shape[1]))\n",
    "\n",
    "# process the results\n",
    "values = dict([(m.fullName,m.getValue()) for m in basehuman.modifiers])\n",
    "data = np.array([values.get(t,0) for t in target_dict.values()],dtype=np.float32)\n",
    "data\n",
    "with tables.open_file(y_file, 'w') as xfo:\n",
    "    atom = tables.Atom.from_dtype(data.dtype)\n",
    "    # create an expandable array\n",
    "    data_storage = xfo.create_earray(xfo.root, 'data', atom, (0,data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T19:35:54.771079",
     "start_time": "2016-11-17T19:35:49.571563"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make base human model\n",
    "with mhpath: \n",
    "    basehuman = getHuman()\n",
    "    \n",
    "    # rig\n",
    "    if rigfile:\n",
    "        humanargparser.addRig(basehuman,rigfile)\n",
    "\n",
    "    if proxyname:\n",
    "        # load proxy\n",
    "        pxy = proxy.loadTextProxy(basehuman, proxyfile, type='Proxymesh')\n",
    "        mesh,obj = pxy.loadMeshAndObject(basehuman)\n",
    "        basehuman.setProxy(pxy)\n",
    "\n",
    "        # _adaptProxyToHuman\n",
    "        mesh = obj.getSeedMesh() #  just gets proxy mesh\n",
    "        pxy.update(mesh) # mesh change cords and calc normals\n",
    "        mesh.update() # calc norms, coorgs and tangents\n",
    "    \n",
    "    randomValues=rand_modifier_values()\n",
    "    params=modifiers2params(randomValues)\n",
    "    params = np.ones_like(params)/2 # grab middle values\n",
    "    randomValues=params2modifiers(params)\n",
    "    human.updateMacroModifiers()\n",
    "    basehuman = assignModifierValues(basehuman, randomValues)\n",
    "    \n",
    "    mesh = basehuman.meshData\n",
    "    group_mask = np.ones(len(mesh._faceGroups), dtype=bool)\n",
    "    face_mask = group_mask[mesh.group]\n",
    "    basehuman._staticFaceMask = face_mask\n",
    "    basehuman.meshData.changeFaceMask(basehuman.staticFaceMask)\n",
    "    basehuman.meshData.updateIndexBufferFaces()\n",
    "    basehuman.changeVertexMask(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T20:37:48.063974",
     "start_time": "2016-11-17T19:35:54.772966"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|##########| 998/998 100% [elapsed: 1:01:53 left: 00:00,  0.27 iters/sec]"
     ]
    }
   ],
   "source": [
    "# make the edge cases mixed with some random ones\n",
    "nb_targets=len(target_dict)\n",
    "edge_params = np.ones((2*nb_targets+1,nb_targets))/2\n",
    "for i in range(nb_targets):\n",
    "    edge_params[i,i]=1.0\n",
    "    edge_params[nb_targets+i,i]=0.0\n",
    "    \n",
    "# add some random ones and shuffle\n",
    "edge_params = np.concatenate([edge_params,np.random.random(edge_params.shape)])\n",
    "np.random.shuffle(edge_params)\n",
    "\n",
    "for params in tqdm(edge_params, leave=True):\n",
    "    # fresh args but re-use the old human to save time initialising\n",
    "    with mhpath: \n",
    "\n",
    "        human = getHuman()\n",
    "\n",
    "        if proxyname:\n",
    "            # load proxy\n",
    "            pxy = proxy.loadTextProxy(human, proxyfile, type='Proxymesh')\n",
    "            mesh,obj = pxy.loadMeshAndObject(human)\n",
    "            human.setProxy(pxy)\n",
    "\n",
    "            # _adaptProxyToHuman\n",
    "            mesh = obj.getSeedMesh() #  just gets proxy mesh\n",
    "            pxy.update(mesh) # mesh change cords and calc normals\n",
    "            mesh.update() # calc norms, coorgs and tangents\n",
    "\n",
    "        randomValues = params2modifiers(params)\n",
    "        human = assignModifierValues(human, randomValues)\n",
    "        morphTarget = -(basehuman.mesh.coord-human.mesh.coord)       \n",
    "        \n",
    "        \n",
    "        # append the results to a file as float32\n",
    "        with tables.open_file(X_file, 'a') as xfo:\n",
    "            xfo.root.data.append([morphTarget])\n",
    "        with tables.open_file(y_file, 'a') as xfo:\n",
    "            xfo.root.data.append([params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T13:27:37.967441",
     "start_time": "2016-11-18T13:27:37.894683"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rv=np.array([m[1] for m in sorted([(m.fullName,m.getValue()) for m in human.modifiers])])\n",
    "rv2=np.array([m[1] for m in sorted(randomValues.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T13:30:15.028052",
     "start_time": "2016-11-18T13:30:15.024830"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(human.targetsDetailStack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-18T13:26:18.448176",
     "start_time": "2016-11-18T07:52:39.380602"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|##--------| 4276/16000  26% [elapsed: 5:33:36 left: 15:14:42,  0.21 iters/sec]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e9bc77bb43d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mmhpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mhuman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetHuman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproxyname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1000bac520e3>\u001b[0m in \u001b[0;36mgetHuman\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mhumanmodifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmods_loaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         modifiers = humanmodifier.loadModifiers(\n\u001b[0;32m---> 24\u001b[0;31m             getpath.getSysDataPath('modifiers/modeling_modifiers.json'), human)\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/humanmodifier.pyc\u001b[0m in \u001b[0;36mloadModifiers\u001b[0;34m(filename, human)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhuman\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0mmodifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetHuman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loaded %s modifiers from file %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/humanmodifier.pyc\u001b[0m in \u001b[0;36msetHuman\u001b[0;34m(self, human)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetHuman\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuman\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mhuman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddModifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/human.pyc\u001b[0m in \u001b[0;36maddModifier\u001b[0;34m(self, modifier)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misMacro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateMacroModifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetModifierDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/human.pyc\u001b[0m in \u001b[0;36mupdateMacroModifiers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodifiers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misMacro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m                 \u001b[0mmodifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/humanmodifier.pyc\u001b[0m in \u001b[0;36msetValue\u001b[0;34m(self, value, skipDependencies)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclampValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdateModifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMacroModifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipDependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclampValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/humanmodifier.pyc\u001b[0m in \u001b[0;36msetValue\u001b[0;34m(self, value, skipDependencies)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0mtWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetTargetWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtWeight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtWeights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetDetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtWeight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mskipDependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/apps/human.pyc\u001b[0m in \u001b[0;36msetDetail\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msetDetail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanonicalPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargetsDetailStack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/lib/getpath.pyc\u001b[0m in \u001b[0;36mcanonicalPath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mUseful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomparing\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mformatPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlocalPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/lib/getpath.pyc\u001b[0m in \u001b[0;36mformatPath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpathToUnicode\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcanonicalPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/posixpath.pyc\u001b[0m in \u001b[0;36mnormpath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0minitial_slashes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;31m# POSIX allows one or two initial slashes, but treats three or more\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# as single slash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# now make random targets and save to file\n",
    "for t in tqdm(range(16000), leave=True):\n",
    "    # fresh args but re-use the old human to save time initialising\n",
    "    with mhpath: \n",
    "\n",
    "        human = getHuman()\n",
    "\n",
    "        if proxyname:\n",
    "            # load proxy\n",
    "            pxy = proxy.loadTextProxy(human, proxyfile, type='Proxymesh')\n",
    "            mesh,obj = pxy.loadMeshAndObject(human)\n",
    "            human.setProxy(pxy)\n",
    "\n",
    "            # _adaptProxyToHuman\n",
    "            mesh = obj.getSeedMesh() #  just gets proxy mesh\n",
    "            pxy.update(mesh) # mesh change cords and calc normals\n",
    "            mesh.update() # calc norms, coorgs and tangents\n",
    "\n",
    "        # set random targets\n",
    "        randomValues=rand_modifier_values()\n",
    "        params=modifiers2params(randomValues)\n",
    "        human = assignModifierValues(human, randomValues)\n",
    "        \n",
    "        # process the results\n",
    "#         values = dict([(m.fullName,m.getValue()) for m in human.modifiers])\n",
    "#         params = np.array([values.get(t,0) for t in target_dict.values()],dtype=np.float32)\n",
    "        # normalise the params? shift to 0-1 except for metadata ones which already are\n",
    "#         params = normalise_params(params)\n",
    "        morphTarget = -(basehuman.mesh.coord-human.mesh.coord)       \n",
    "        \n",
    "        \n",
    "        # append the results to a file as float32\n",
    "        with tables.open_file(X_file, 'a') as xfo:\n",
    "            xfo.root.data.append([morphTarget])\n",
    "        with tables.open_file(y_file, 'a') as xfo:\n",
    "            xfo.root.data.append([params])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:52.164896",
     "start_time": "2016-11-17T14:53:52.102690"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# export SKELETON\n",
    "skeleton=basehuman.getSkeleton()\n",
    "\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "\n",
    "import transformations as tm\n",
    "\n",
    "## add to metadata\n",
    "jsondata = OrderedDict()\n",
    "if \"metadata\" not in jsondata:\n",
    "    jsondata[\"metadata\"]={}\n",
    "skeletonMetadata=jsondata[\"metadata\"][\"skeleton\"]={ \"name\": skeleton.name,\n",
    "                         \"version\": skeleton.version,\n",
    "                         \"description\": skeleton.description,\n",
    "                         \"plane_map_strategy\": skeleton.plane_map_strategy,\n",
    "                         \"license\": skeleton.license.asDict(),\n",
    "                       }\n",
    "\n",
    "lastPos=skeleton.getBones()[0].headPos\n",
    "# bones\n",
    "bones = []\n",
    "for bone in skeleton.getBones():\n",
    "    bonedef = {}\n",
    "    bonedef[\"name\"]=bone.name\n",
    "    \n",
    "    if bone.parent:\n",
    "        bonedef[\"parent\"] = bone.parent.index\n",
    "    else:\n",
    "        bonedef[\"parent\"] = -1\n",
    "    \n",
    "    # distortion form res, [0,0,0,1] is null in threejs\n",
    "    bonedef[\"rotq\"] = [0, 0, 0, 1]\n",
    "    \n",
    "    # tail position relative to parent\n",
    "    if bone.parent:\n",
    "        bonedef[\"pos\"]= (bone.tailPos-bone.parent.tailPos).tolist()\n",
    "    else:\n",
    "        bonedef[\"pos\"]= bone.tailPos.tolist()\n",
    "        # adjust for offset in center of geom? nope\n",
    "        # bonedef[\"pos\"][1]-=human.getBoundingBox().mean(0)[1]\n",
    "    \n",
    "    # scale\n",
    "    bonedef[\"scl\"]= [skeleton.scale,skeleton.scale,skeleton.scale] \n",
    "\n",
    "    bones.append(bonedef)\n",
    "jsondata[\"bones\"] = bones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:52.757801",
     "start_time": "2016-11-17T14:53:52.734877"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "world_tail_pos={}\n",
    "for bone in bones:\n",
    "    pos=np.array(bone[\"pos\"])\n",
    "    if bone[\"parent\"]!=-1:\n",
    "        parent = bones[bone[\"parent\"]]\n",
    "        pos+=world_tail_pos[parent['name']]\n",
    "    world_tail_pos[bone['name']]=pos\n",
    "df=pd.DataFrame.from_dict(world_tail_pos,orient=\"index\")\n",
    "df.columns=['x','y','z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T08:40:26.631517",
     "start_time": "2016-11-16T08:40:26.605659"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:53.473451",
     "start_time": "2016-11-17T14:53:53.255351"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save weights and inds\n",
    "# the data is indexes by bones but we need to reorder it by vertex, so first compile the data\n",
    "\n",
    "# get weights for the proxied mesh\n",
    "if proxyname:\n",
    "    vertex_weights=basehuman.mesh.getVertexWeights(basehuman.proxy.getVertexWeights(skeleton.getVertexWeights()))\n",
    "else:\n",
    "    vertex_weights=basehuman.mesh.getVertexWeights(skeleton.getVertexWeights())\n",
    "\n",
    "from collections import defaultdict\n",
    "vwdata=defaultdict(dict)\n",
    "for bone in skeleton.getBones():\n",
    "    if bone.name in vertex_weights.data:\n",
    "        for vert,w in zip(*vertex_weights.data[bone.name]):\n",
    "            vwdata[vert][bone.index]=w\n",
    "\n",
    "\n",
    "# now we can export them\n",
    "LOST_CONNECTIONS=0\n",
    "\n",
    "jsondata[\"influencesPerVertex\"] = influencesPerVertex = 4 # max unless I change skinnedMesh\n",
    "skinIndices=[]\n",
    "skinWeights=[]\n",
    "for vert in range(vertex_weights.vertexCount):\n",
    "    vw=vwdata[vert].items()\n",
    "    a=np.array(vw).T\n",
    "    a.sort(-1) # sort by weight\n",
    "    \n",
    "    if a.shape[1]>influencesPerVertex:\n",
    "        LOST_CONNECTIONS+=a.shape[1]-influencesPerVertex\n",
    "    \n",
    "    # pad and crop\n",
    "    pca=np.zeros((2,influencesPerVertex), dtype=float)\n",
    "    min_size=min(pca.shape[1],a.shape[1])\n",
    "    pca[:2,:min_size]=a[:2,:min_size]\n",
    "    \n",
    "    skinIndices+=pca[0,:].tolist()\n",
    "    skinWeights+=pca[1,:].tolist()\n",
    "jsondata[\"skinWeights\"] = skinWeights\n",
    "jsondata[\"skinIndices\"] = skinIndices\n",
    "\n",
    "assert len(skinWeights)==len(skinIndices),'Should have equal number of inds and weights'\n",
    "assert vertex_weights.vertexCount*influencesPerVertex==len(skinIndices),'should have 4 times as many inds as vertices'\n",
    "assert max(skinIndices)<vertex_weights.vertexCount, 'vertex index should not refer to more vertices than we have'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-16T08:34:25.149546",
     "start_time": "2016-11-16T08:34:25.147555"
    }
   },
   "source": [
    "# export base human while masking helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:53.997073",
     "start_time": "2016-11-17T14:53:53.639534"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/vendor/makehuman-commandline/makehuman/data/3dobjs/base.obj\n"
     ]
    }
   ],
   "source": [
    "# combine base and morphs while giving helpers a diff index\n",
    "\n",
    "## TODO make this iterative\n",
    "\n",
    "# The base object has invisible groups but they are not parsed by json loader so I need to\n",
    "# change it to add materials for each group, and a matrial index. This will let me hide them in three js\n",
    "# at the moment they are just groups\n",
    "\n",
    "# inputs\n",
    "if proxyfile:\n",
    "    # use the proxy obj as it might have groups in which we want\n",
    "    infile=mhpath+'/'+proxyfile.replace('.proxy','.obj')\n",
    "else:\n",
    "    infile=mhpath+'/data/3dobjs/base.obj'\n",
    "#     infile=baseargs[\"outputs\"][0]\n",
    "print infile\n",
    "from collections import Counter\n",
    "from wrap_mh.convert.convert_obj_three import (ALIGN, SHADING, BAKE_COLORS, TRUNCATE, TEMPLATE_FILE_ASCII, SCALE, \n",
    "    generate_materials_string, generate_morph_colors, extract_material_colors, generate_uv, generate_normal,\n",
    "    generate_face, generate_face, generate_color_decimal, generate_vertex, generate_morph_targets, get_name,\n",
    "    parse_obj)\n",
    "\n",
    "morphfilesa=[]\n",
    "morphfilesa\n",
    "morphfiles=' '.join(morphfilesa)\n",
    "colorfiles=''\n",
    "outfile=os.path.abspath(os.path.join(\"output\",\"json\",runname+\"_mh2obh2json.json\"))\n",
    "\n",
    "# start by parsing base mh obj\n",
    "faces, vertices, uvs, normals, materials, mtllib = wrap_mh.convert.convert_obj_three.parse_obj(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:54.033724",
     "start_time": "2016-11-17T14:53:53.998585"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# modify the base obj\n",
    "#################################\n",
    "\n",
    "# order unique groups by how common they are\n",
    "groupCounts = Counter([f['group'] for f in faces])\n",
    "groups = [g[0] for g in groupCounts.most_common()]\n",
    "\n",
    "# set material index\n",
    "for i in range(len(faces)):\n",
    "    group = faces[i][\"group\"]\n",
    "    material = groups.index(group)\n",
    "    faces[i][\"material\"] = material\n",
    "\n",
    "# and make those materials\n",
    "materials={}\n",
    "for group in groups:\n",
    "    materialInd = groups.index(group)\n",
    "    materials[group]=materialInd\n",
    "#     {\n",
    "#         \"DbgColor\" : 15658734,\n",
    "#         \"DbgIndex\" : materialInd,\n",
    "#         \"DbgName\" : group,\n",
    "#         \"colorDiffuse\" : [0.5903, 0.44, 0.338],\n",
    "#         \"colorSpecular\" : [0.3, 0.3, 0.3],\n",
    "#         \"opacity\" : group!=\"body\"\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:54.113323",
     "start_time": "2016-11-17T14:53:54.070410"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "# now regular loading from parse_ascii\n",
    "#########################################3\n",
    "n_vertices = len(vertices)\n",
    "n_faces = len(faces)\n",
    "\n",
    "# align model\n",
    "if ALIGN == \"center\":\n",
    "    center(vertices)\n",
    "elif ALIGN == \"centerxz\":\n",
    "    centerxz(vertices)\n",
    "elif ALIGN == \"bottom\":\n",
    "    bottom(vertices)\n",
    "elif ALIGN == \"top\":\n",
    "    top(vertices)\n",
    "\n",
    "# generate normals string\n",
    "nnormal = 0\n",
    "normals_string = \"\"\n",
    "if SHADING == \"smooth\":\n",
    "    normals_string = \",\".join(generate_normal(n) for n in normals)\n",
    "    nnormal = len(normals)\n",
    "\n",
    "# extract face groups\n",
    "fgs = [f['group'] for f in faces]\n",
    "groups = list(set(fgs))\n",
    "faceGroups = [groups.index(g) for g in fgs]\n",
    "\n",
    "# extract morph vertices\n",
    "# chance of MEMMORY ERROR here. Might need to append this to the file part by art. Or load morph seperatley\n",
    "morphTargets = generate_morph_targets(morphfiles, n_vertices, infile)\n",
    "\n",
    "# do a test to check that morphtargets are diff here\n",
    "\n",
    "# extract morph colors\n",
    "morphColors, colorFaces, materialColors = generate_morph_colors(colorfiles, n_vertices, n_faces)\n",
    "\n",
    "# TODO\n",
    "animations=[]\n",
    "\n",
    "# generate colors string\n",
    "\n",
    "ncolor = 0\n",
    "colors_string = \"\"\n",
    "\n",
    "if len(colorFaces) < len(faces):\n",
    "    colorFaces = faces\n",
    "    materialColors = extract_material_colors(materials, mtllib, infile)\n",
    "\n",
    "if BAKE_COLORS:\n",
    "    colors_string = \",\".join(generate_color_decimal(c) for c in materialColors)\n",
    "    ncolor = len(materialColors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-17T14:53:54.880305",
     "start_time": "2016-11-17T14:53:54.414794"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19158 vertices, 18486 faces, 139 materials, 0 morph targets. To /media/isisilon/Data/My_Documents/Documents/eclipse-workspace/bb2/mhwebui2/mhwebui2/scripts/output/20161117-133700_None_vae_data/basemodel.json\n"
     ]
    }
   ],
   "source": [
    "# transform morph targets names from data_targets_asym_asym-eye-5-l.target => data/targets/asym/asym-eye-5-l.target being wary of doulbe underscores we don't want to replace\n",
    "# since we need the slashes in the end product but can't have them in the interim product else they would be interpreted as dirs\n",
    "# and stripped. TODO proboboly best to seperate target paths and names anyway\n",
    "morphTargets = [m.replace('__','+').replace('_','/').replace('+','_') for m in morphTargets]\n",
    "\n",
    "\n",
    "# now make dict to export as json\n",
    "\n",
    "\n",
    "TEMPLATE_FILE_ASCII = u\"\"\"{\n",
    "\n",
    "    \"metadata\" :\n",
    "    {\n",
    "        \"name\"          : \"%(name)s\",\n",
    "        \"formatVersion\" : 3.1,\n",
    "        \"sourceFile\"    : \"%(fname)s\",\n",
    "        \"generatedBy\"   : \"OBJConverter\",\n",
    "        \"vertices\"      : %(nvertex)d,\n",
    "        \"faces\"         : %(nface)d,\n",
    "        \"normals\"       : %(nnormal)d,\n",
    "        \"colors\"        : %(ncolor)d,\n",
    "        \"uvs\"           : %(nuv)d,\n",
    "        \"materials\"     : %(nmaterial)d,\n",
    "        \"morphTargets\"  : %(nmorphs)d,\n",
    "        \"bones\"         : %(nbones)d,\n",
    "        \"license\"       : %(license)s,\n",
    "        \"skeleton\"      : %(skeletonMetadata)s,\n",
    "        \"proxymesh\"     : %(proxyMetadata)s\n",
    "    },\n",
    "\n",
    "    \"scale\" : %(scale)f,\n",
    "\n",
    "    \"materials\": [%(materials)s],\n",
    "\n",
    "    \"vertices\": [%(vertices)s],\n",
    "\n",
    "    \"morphTargets\": [%(morphTargets)s],\n",
    "\n",
    "    \"morphColors\": [%(morphColors)s],\n",
    "\n",
    "    \"normals\": [%(normals)s],\n",
    "\n",
    "    \"colors\": [%(colors)s],\n",
    "\n",
    "    \"uvs\": [[%(uvs)s]],\n",
    "\n",
    "    \"faces\": [%(faces)s],\n",
    "\n",
    "    \"bones\" : %(bones)s,\n",
    "    \n",
    "    \"skinWeights\": %(skinWeights)s,\n",
    "    \n",
    "    \"skinIndices\": %(skinIndices)s,\n",
    "    \n",
    "    \"influencesPerVertex\": %(influencesPerVertex)d,\n",
    "\n",
    "    \"animations\" : [%(animations)s]\n",
    "\n",
    "}\"\"\"\n",
    "\n",
    "text =  TEMPLATE_FILE_ASCII % {\n",
    "    \"name\"      : get_name(outfile),\n",
    "    \"fname\"     : os.path.basename(infile),\n",
    "    \"nvertex\"   : len(vertices),\n",
    "    \"nface\"     : len(faces),\n",
    "    \"nuv\"       : len(uvs),\n",
    "    \"nnormal\"   : nnormal,\n",
    "    \"ncolor\"    : ncolor,\n",
    "    \"nmaterial\" : len(materials),\n",
    "    \"nmorphs\":len(morphTargets),\n",
    "    \"nbones\": len(bones),\n",
    "    \"license\": json.dumps(mh_licence.asDict()),\n",
    "    \"skeletonMetadata\": json.dumps(skeletonMetadata),\n",
    "    \"proxyMetadata\": json.dumps(proxyMetadata),\n",
    "\n",
    "    \"materials\" : generate_materials_string(materials, mtllib, infile),\n",
    "\n",
    "    \"normals\"       : normals_string,\n",
    "    \"colors\"        : colors_string,\n",
    "    \"uvs\"           : \",\".join(generate_uv(uv) for uv in uvs),\n",
    "    \"vertices\"      : \",\".join(generate_vertex(v, TRUNCATE, SCALE) for v in vertices),\n",
    "\n",
    "    \"morphTargets\"  : \"\\n%s\\n\\t\" % \",\\n\".join(morphTargets),\n",
    "    \"morphColors\"   : morphColors,\n",
    "\n",
    "    \"faces\"     : \",\".join(generate_face(f, fc) for f, fc in zip(faces, colorFaces)),\n",
    "\n",
    "    \"scale\"    : SCALE,\n",
    "\n",
    "    \"bones\": json.dumps(bones),\n",
    "\n",
    "    \"animations\": \",\".join(animations),\n",
    "    \n",
    "    \"skinWeights\": json.dumps(skinWeights),\n",
    "    \"skinIndices\": json.dumps(skinIndices),\n",
    "    \"influencesPerVertex\":influencesPerVertex\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "baseoutfile=outputdir.joinpath('basemodel.json')\n",
    "out = open(baseoutfile, \"w\")\n",
    "out.write(text)\n",
    "out.close()\n",
    "\n",
    "s=json.dumps(jsondata,indent=4)\n",
    "json.loads(s)\n",
    "\n",
    "print \"%d vertices, %d faces, %d materials, %d morph targets. To %s\" % (len(vertices), len(faces), len(materials), len(morphTargets), baseoutfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6.0,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}